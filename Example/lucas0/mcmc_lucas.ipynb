{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a052ba9d-35b4-4af9-ab8f-ab285216001e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "\n",
    "from pgmpy.estimators import HillClimbSearch\n",
    "from pgmpy.estimators import BDeuScore\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from graphviz import Digraph\n",
    "from itertools import combinations\n",
    "from IPython.display import clear_output\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "import cloudpickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05d06954-6437-409c-b231-d33e86256cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:23,  4.27it/s]\n"
     ]
    }
   ],
   "source": [
    "networks = dict()\n",
    "df_path = './cross_validated_datasets/lucas0/1000/'\n",
    "\n",
    "for i, file in tqdm(enumerate([*filter(lambda x: 'lucas' in x, os.listdir(df_path))])):\n",
    "    data = pd.read_csv(df_path + file)\n",
    "    \n",
    "    networks[i] = list()\n",
    "\n",
    "    scoring_method = BDeuScore(data=data)    \n",
    "    est = HillClimbSearch(data=data)\n",
    "    networks[i].append(est.estimate(scoring_method=scoring_method, max_indegree=2, max_iter=int(1e4), show_progress=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54a92b5e-752b-4f5f-a487-643bf565c64e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 34094.49it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(df_path + 'lucas0_1000_0.csv')\n",
    "edges = [*combinations(data.columns, 2)]\n",
    "\n",
    "observed_networks = [[] for i in range(len(networks))]\n",
    "\n",
    "for i, file in tqdm(enumerate([*filter(lambda x: 'lucas' in x, os.listdir(df_path))])):\n",
    "    for j, network in enumerate(networks[i]):\n",
    "        observed_networks[i].append([])\n",
    "        \n",
    "        for edge in edges:\n",
    "            if (edge in network.edges):\n",
    "                observed_networks[i][j].append(-1)\n",
    "            elif ((edge[1], edge[0]) in network.edges):\n",
    "                observed_networks[i][j].append(1)\n",
    "            else: \n",
    "                observed_networks[i][j].append(0)\n",
    "\n",
    "observed_networks = [np.array(x) for x in observed_networks]\n",
    "observed_networks = np.array(observed_networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "745a19be-e236-4239-bba6-fbd575f6252c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values = [-1, 0, 1]\n",
    "k = len(values)          \n",
    "n = len(networks)\n",
    "total_count = len(observed_networks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fee43f28-8d9b-4a30-901f-c8076f715efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_edge_frequency(values, observed_networks, edge_index):\n",
    "    \"\"\"\n",
    "    values: list of possible values for each edge (ex: [0, 1, 2] or [-1, 0, 1])\n",
    "    observed_networks: list of networks observed\n",
    "    edge_index: edged which will be counted\n",
    "    \"\"\"\n",
    "    \n",
    "    observations = list()\n",
    "    \n",
    "    for edge_list in observed_networks:\n",
    "        edges_obs = edge_list[:, edge_index]\n",
    "\n",
    "        rightEdge = np.sum(edges_obs == values[0])\n",
    "        noEdge = np.sum(edges_obs == values[1])\n",
    "        leftEdge = np.sum(edges_obs == values[2])\n",
    "\n",
    "        observations.append([rightEdge, noEdge, leftEdge])\n",
    "    \n",
    "    return observations\n",
    "\n",
    "\n",
    "def learn_edge(edge, edge_index, chains=10, tune=10000, draws=10000, nthreads=10):\n",
    "    observed_edges = get_edge_frequency(values, observed_networks, edge_index)\n",
    "\n",
    "    with pm.Model() as model_dm_explicit:\n",
    "        frac = pm.Dirichlet(\"frac\", a=np.ones(k))\n",
    "        conc = pm.Lognormal(\"conc\", mu=1, sigma=1)\n",
    "        counts = pm.DirichletMultinomial(\n",
    "            \"counts\", n=total_count, a=frac * conc, shape=(n, k), observed=observed_edges\n",
    "        )\n",
    "\n",
    "        trace_dm_explicit = pm.sample(chains=chains, tune=tune, draws=draws, step=pm.NUTS(), return_inferencedata=False, cores=nthreads)\n",
    "\n",
    "        Model = cloudpickle.dumps({\n",
    "                'model': model_dm_explicit, \n",
    "                'frac':  frac, \n",
    "                'conc': conc, \n",
    "                'counts': counts, \n",
    "                'trace': trace_dm_explicit\n",
    "            })\n",
    "    \n",
    "        file = open('./mcmc/lucas0_10000' + str(chains) + '_t' + str(tune) + '_d'+ str(draws) + '_mcmc_' + edge[0]+\"-\"+edge[1] + '.pickle', 'wb')\n",
    "        file.write(Model)\n",
    "        \n",
    "        return trace_dm_explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e2b1a2a-8b63-4fe9-880f-0af9cdb7b37d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/66 [00:00<?, ?it/s]Multiprocess sampling (10 chains in 10 jobs)\n",
      "NUTS: [frac, conc]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='50000' class='' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [50000/50000 00:04&lt;00:00 Sampling 10 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 10 chains for 2_500 tune and 2_500 draw iterations (25_000 + 25_000 draws total) took 5 seconds.\n",
      "  0%|                                                    | 0/66 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can only convert xarray dataarray, xarray dataset, dict, netcdf filename, numpy array, pystan fit, emcee fit, pyro mcmc fit, numpyro mcmc fit, cmdstan fit csv filename, cmdstanpy fit to InferenceData, not MultiTrace",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, edge \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(edges)):\n\u001b[0;32m----> 6\u001b[0m     models[edge[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39medge[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mlearn_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdraws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m end \u001b[38;5;241m=\u001b[39m timer()\n",
      "Cell \u001b[0;32mIn[17], line 46\u001b[0m, in \u001b[0;36mlearn_edge\u001b[0;34m(edge, edge_index, chains, tune, draws, nthreads)\u001b[0m\n\u001b[1;32m     43\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./mcmc/lucas0_10000\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(chains) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_t\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(tune) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(draws) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_mcmc_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m edge[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39medge[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(Model)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrace_dm_explicit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pymc_env/lib/python3.11/site-packages/arviz/stats/stats.py:1358\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(data, var_names, filter_vars, group, fmt, kind, round_to, circ_var_names, stat_focus, stat_funcs, extend, hdi_prob, skipna, labeller, coords, index_origin, order)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInferenceData does not contain group: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1358\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mposterior\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1359\u001b[0m var_names \u001b[38;5;241m=\u001b[39m _var_names(var_names, dataset, filter_vars)\n\u001b[1;32m   1360\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset \u001b[38;5;28;01mif\u001b[39;00m var_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dataset[var_names]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pymc_env/lib/python3.11/site-packages/arviz/data/converters.py:176\u001b[0m, in \u001b[0;36mconvert_to_dataset\u001b[0;34m(obj, group, coords, dims)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_dataset\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, group\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposterior\u001b[39m\u001b[38;5;124m\"\u001b[39m, coords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a supported object to an xarray dataset.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    This function is idempotent, in that it will return xarray.Dataset functions\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    xarray.Dataset\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m     inference_data \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_inference_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(inference_data, group, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pymc_env/lib/python3.11/site-packages/arviz/data/converters.py:130\u001b[0m, in \u001b[0;36mconvert_to_inference_data\u001b[0;34m(obj, group, coords, dims, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     allowable_types \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxarray dataarray\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxarray dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcmdstanpy fit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    129\u001b[0m     )\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCan only convert \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(allowable_types)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to InferenceData, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m InferenceData(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{group: dataset})\n",
      "\u001b[0;31mValueError\u001b[0m: Can only convert xarray dataarray, xarray dataset, dict, netcdf filename, numpy array, pystan fit, emcee fit, pyro mcmc fit, numpyro mcmc fit, cmdstan fit csv filename, cmdstanpy fit to InferenceData, not MultiTrace"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "models = dict()\n",
    "\n",
    "for i, edge in enumerate(tqdm(edges)):\n",
    "    models[edge[0]+\"-\"+edge[1]] = learn_edge(edge, i, chains=10, tune=2500, draws=2500, nthreads=10)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "\n",
    "end = timer()\n",
    "elapsed_time = timedelta(seconds=end-start)\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc5913-4c0d-434e-8fb4-a4e99046d926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
